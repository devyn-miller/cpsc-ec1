# -*- coding: utf-8 -*-
"""Car Object Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/car-object-detection-7e07bcc6-0590-4f92-93c9-cd69bdaa6283.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240406/auto/storage/goog4_request%26X-Goog-Date%3D20240406T204201Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D066da0c0fc0c7c9130f41f17f21e5c11ef35e569a0c1746b3d6ce8b3a7d977ef0f1a88528a4e719f4eda94b2ffdd932dbac44a043d5c703548d34e71dffb2bbd372f0a28062de860356fc7a679c6fa3417162118feeced1e8326124f29b31f45acf16f3acfefe064d8a5a13c66bdeba0a5a5b2fbd38f27e2ffdd3860c28dc7ba5ac21a45a1c44b964045ca7c16c72942430c854af185e5c52b7df0ae95eee22604ee228cde4ada65cfb2788695a7210606f135ad36f58e1479bed15a11cd4ae0fea97ff7bf25335aecb3ad704e83fe729713b91da3a46d58e849e13c0be130a2991e1fa5a72236b809807fa82f7271216a760b88f4f34e2ab8b0b6d55440af85
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'car-object-detection:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F843852%2F3866417%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240406%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240406T204201Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D47b4552d68d77eb624b58b1d446d262717c3d2c3b532eacdf5222312cf303066af5efa0a7a41d301c2515b7472a0c102c327fda104f1157dc2f1dd1f5c90a41d19ec466d72b08648caf362b49e713373d7b10481c32a35dced5288146f7ca45d22369210f104ac8db3240f845699dbf54172d34d9091244bac937947234a50719795e8ca623f386ce995bbda18236841a4b99e646046f33d8ca2a785f6eb3740252f55bd02a90eb5a75d300c00d5e03a5ab93594b77bf2bea785b9fd49d7c74b4244fedf5109cf240737ca64d2a9f0e60018f5c14efd983fe42a249238e6b8d0d5f66fa5d8d976da579f9925e3024642fbf640143f64488469ffad11eb6532fb'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# Introduction

Welcome to my first computer vision notebook! This notebook covers the setup and training of an object detection model. Specifically, it delves deep into the intricacies of the setup of a from-scratch object detection model. With regard to training, the notebook describes how one can create custom keras callbacks that can be utilized to augment one's knoweldge of the training process.

Without further ado, let's get started!

# Problem Overview

This notebook tackles **single object detection**. As the name suggests, single object detection entails the detection of a single object. More specifically, a model is tasked with providing the coordinates of the bounding box of an object in a particular image.

# Model Overview

Since we are asked to predict a set of numbers (the coordinates of the bounding box of an object), we can treat single object detection as a linear regression problem, except with an image as input. To handle the image, we can utilise a convolutional neural network (CNN).

Now that we have a fairly decent idea about how to approach this problem, we can get started with the setup.

Quick side note: for those unfamiliar with CNNs, read more [here](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53). Additionally, Kaggle has a great course on computer vision, so feel free to check that out for more practical information ([link](https://www.kaggle.com/learn/computer-vision)).

# Setup

As usual, we'll start by importing some libraries.
"""

# Data Manipulation
import numpy as np
import pandas as pd

# Visualization/Image Processing
import cv2
import matplotlib.pyplot as plt

# Machine Learning
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Input, BatchNormalization, Flatten, MaxPool2D, Dense

# Other
from pathlib import Path

"""Next, we'll setup our dataset."""

train_path = Path("../input/car-object-detection/data/training_images")
test_path = Path("../input/car-object-detection/data/testing_images")

"""Since `cv2`, the library we'll be using to plot the bounding boxes and the images, only accepts integer values as vertices, we'll need to convert the coordinates of the bounding boxes to integers.

The dataset contains information about multi-object detection; however, this notebook is concerned with single object detection. To account for this slight discrepancy, we will omit the duplicate values of the `image` column. This results in each image having only one corresponding set of bounding box coordinates.
"""

train = pd.read_csv("../input/car-object-detection/data/train_solution_bounding_boxes (1).csv")
train[['xmin', 'ymin', 'xmax', 'ymax']] = train[['xmin', 'ymin', 'xmax', 'ymax']].astype(int)
train.drop_duplicates(subset='image', inplace=True, ignore_index=True)

"""Next, I'll create some utility functions that make it easy to display images from files and dataframes."""

def display_image(img, bbox_coords=[], pred_coords=[], norm=False):
    # if the image has been normalized, scale it up
    if norm:
        img *= 255.
        img = img.astype(np.uint8)

    # Draw the bounding boxes
    if len(bbox_coords) == 4:
        xmin, ymin, xmax, ymax = bbox_coords
        img = cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 3)

    if len(pred_coords) == 4:
        xmin, ymin, xmax, ymax = pred_coords
        img = cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (255, 0, 0), 3)

    plt.imshow(img)
    plt.xticks([])
    plt.yticks([])

def display_image_from_file(name, bbox_coords=[], path=train_path):
    img = cv2.imread(str(path/name))
    display_image(img, bbox_coords=bbox_coords)

def display_from_dataframe(row, path=train_path):
    display_image_from_file(row['image'], bbox_coords=(row.xmin, row.ymin, row.xmax, row.ymax), path=path)


def display_grid(df=train, n_items=3):
    plt.figure(figsize=(20, 10))

    # get 3 random entries and plot them in a 1x3 grid
    rand_indices = [np.random.randint(0, df.shape[0]) for _ in range(n_items)]

    for pos, index in enumerate(rand_indices):
        plt.subplot(1, n_items, pos + 1)
        display_from_dataframe(df.loc[index, :])

"""**A quick formatting note**: the green rectangle represents the correct bounding whereas the red rectangle represents the predicted bounding box. This convention is used throughout this notebook."""

display_image_from_file("vid_4_10520.jpg")

display_grid()

"""The functions work perfectly, and the logic for the bounding boxes seems to be flawless. We can now move on to creating and training the model.

# Model Training

## Data Generator

Before training the model, we must define a generator that keras accepts. If you're not familiar with python generators or are in need of a quick refresher, check out [this resource](https://www.programiz.com/python-programming/generator).

In keras, all we need to do is initialize some arrays containing images and their corresponding bounding box coordinates. Then, we simply return the newly-created arrays in a dictionary.
"""

def data_generator(df=train, batch_size=16, path=train_path):
    while True:
        images = np.zeros((batch_size, 380, 676, 3))
        bounding_box_coords = np.zeros((batch_size, 4))

        for i in range(batch_size):
                rand_index = np.random.randint(0, train.shape[0])
                row = df.loc[rand_index, :]
                images[i] = cv2.imread(str(train_path/row.image)) / 255.
                bounding_box_coords[i] = np.array([row.xmin, row.ymin, row.xmax, row.ymax])

        yield {'image': images}, {'coords': bounding_box_coords}

"""The dictionary keys are crucial, as keras needs them to locate the correct input/output."""

# Test the generator
example, label = next(data_generator(batch_size=1))
img = example['image'][0]
bbox_coords = label['coords'][0]

display_image(img, bbox_coords=bbox_coords, norm=True)

"""## Model Building

I'll use keras' functional API as it's incredibly easy to utilize custom inputs and predict custom outputs. Specifically, I'll use a fairly large neural network to start out with, and adjust the parameters of the layers if necessary.

Notice that the dictionary keys in the generator correspond to the names of the input and output layers.
"""

input_ = Input(shape=[380, 676, 3], name='image')

x = input_

for i in range(10):
    n_filters = 2**(i + 3)
    x = Conv2D(n_filters, 3, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPool2D(2, padding='same')(x)

x = Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dense(32, activation='relu')(x)
output = Dense(4, name='coords')(x)

model = tf.keras.models.Model(input_, output)
model.summary()

"""100 million parameters! I suppose that's fine for now; although, if I consider exporting this model, I will be left with a very very large file.

Moving on, we'll compile the model.

For each output, we need to specify a loss and a metric. To do this, we simply reference the dictionary key used in the generator and assign it our desired loss function/metric.
"""

model.compile(
    loss={
        'coords': 'mse'
    },
    optimizer=tf.keras.optimizers.Adam(1e-3),
    metrics={
        'coords': 'accuracy'
    }
)

"""Before we actually train the model, let's define a callback that tests the current model on three, randomly selected images."""

# Some functions to test the model. These will be called every epoch to display the current performance of the model
def test_model(model, datagen):
    example, label = next(datagen)

    X = example['image']
    y = label['coords']

    pred_bbox = model.predict(X)[0]

    img = X[0]
    gt_coords = y[0]

    display_image(img, pred_coords=pred_bbox, norm=True)

def test(model):
    datagen = data_generator(batch_size=1)

    plt.figure(figsize=(15,7))
    for i in range(3):
        plt.subplot(1, 3, i + 1)
        test_model(model, datagen)
    plt.show()

class ShowTestImages(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        test(self.model)

"""We'll quickly use these methods to evaluate the current performance of our model."""

test(model)

"""The model isn't great; in fact, its predictions aren't even visible.

But, the model's poor performance is expected as we haven't even trained the model yet.
So, let's do just that.

We'll use Kaggle's GPU to train the model in order to drastically speed up the training process.
"""

with tf.device('/GPU:0'):
    _ = model.fit(
        data_generator(),
        epochs=9,
        steps_per_epoch=500,
        callbacks=[
            ShowTestImages(),
        ]
    )

"""The model is doing quite well; the `MSE` is relatively low and the accuracy is very high.

Since the model training seems to be complete, we can now export the model and store it for later use.
"""

model.save('car-object-detection.h5')

"""# Conclusion
This notebook described the setup of a single object detection model, along with the training process. Moreover, this notebook provided some insight on how to deal with datasets that aren't necessarily in a format compatible with the default keras settings. Lastly, this notebook described the thinking that goes behind approaching a computer vision problem such as single object detection.

For those eager to dive deeper, here are some more computer vision projects:
 - Image Segmentation
 - Single Object Detection (with videos)
 - Multiple Object Detection (with images)
 - Multiple Object Detection (with videos)
 - Object Detection and Classification (with images and videos)

If you found this notebook helpful, consider upvoting. Upvotes allow this notebook to circulate around Kaggle, thus resulting in the notebook reaching and helping more people.

### Thanks for Reading!
"""